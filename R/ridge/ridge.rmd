---
title: "Explore the ridge principle and ridge regression nuances"
author: "Edoardo Costantini"
output:
  html_document:
    toc: true             # table of content true
    toc_depth: 4          # up to three depths of headings (specified by #, ## and ###)
    number_sections: true # if you want number sections at each table header
    theme: united         # many options for theme
    highlight: tango      # specifies the syntax highlighting style
---

# Preamble

- Project:   knowledgeBase
- Objective: explore the ridge principle and ridge regression nuances
- Author:    Edoardo Costantini
- Created:   2022-02-28
- Modified:  `r format(Sys.time(), '%Y-%m-%d')`
- Notion:    https://lavish-hollyhock-981.notion.site/Ridge-regression-8134d8babda5413ab182df645c6196a8
- Other ref:

# Content

## Introduction

When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. By imposing a size constraint (or penalty) on the coefficients this problem is alleviated.

Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares:

or equivalently

with a one to one correspondence between the $\lambda$ and $t$ parameters.
The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving the minimization problem.

Notice that the intercept $\beta_0$ has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for Y. By centering the predictors inputs, we can separate the solution to the [minimazion problem](https://www.notion.so/Ridge-regression-8134d8babda5413ab182df645c6196a8) into two parts:

- Intercept
$$
\beta_0 = \bar{y}=\frac{1}{N}\sum_{i = 1}^{N} y_i
$$

- Penalised regression coefficinets
$$
\hat{\beta}^{\text{ridge}}=(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^Ty
$$
which is the regular way of estimating regression coefficients with an added penalty term ($\lambda \mathbf{I}$) on the diagonal of the cross-product matrix ($\mathbf{X}^T\mathbf{X}$) to make it invertible ($(...)^{-1}$).

## Code

First let's set up the R environment.
We'll need two packages: `glmnet` and `ridge`.
We'll use them just to check how other people have implemented the same concepts.
For these notes, we will work with the `mtcars` data.
We will use the first column of the dataset (variable named `mpg`) as a dependent variable and the remaining ones as predictors.
```{r, results = 'hide', message = FALSE, warning = FALSE}
# Load packages
library(glmnet)
library(ridge)

# Take the mtcars data
y <- mtcars[, "mpg"]
X <- mtcars[, -1]

# Create a few shorthands we will use
n <- nrow(X)
p <- ncol(X)

```

### Fitting ridge regression manually
First, let's **fit ridge regression** manually by separating the intercept and the regression coefficients estimation (two-step approach):
```{r}
# Scale the data (standardize)
X_scale <- scale(X, center = TRUE, scale = TRUE)

# Compute the cross-product matrix of the data
XtX <- t(X_scale) %*% X_scale

# Define the identify matrix
I <- diag(ncol(X_scale))

# Define a lambda value
lambda <- .1

# Estimate the regression coefficients with the ridge penalty
bs_hat_r <- solve(XtX + lambda * I) %*% t(X_scale) %*% y

# Estimate the intercept
b0_hat_r <- mean(y)

# Print the results
round(
        data.frame(twostep = c(b0 = b0_hat_r, b = bs_hat_r)),
        3
)

```

It is important to note the effect of centering and scaling.
When fitting ridge regression, many sources reccomend to center the data.
This allows to separate the estimation of the intercept from the estimation of the regression coefficients.
As a result, only the regression coefficients are penalised.
To understand the effect of centering, consider what happens in regular OLS estimation when **predictors are centered**:
```{r}
# Create a version of X that is centered
X_center <- scale(X, center = TRUE, scale = FALSE)

# Fit an regular linear model
lm_ols <- lm(y ~ X_center)

# Check that b0 is equal to the mean of y
coef(lm_ols)["(Intercept)"] - mean(y)

```
As for **scaling**, the important thing to not for now is that scaling allows the penalty term to act in the same way independently of the scale of the predictors.

#### An alternative way to avoid penalising the intercept
We can also avoid the penalisation of the intercept by setting to 0 the corresponding element in the `pen` object.
By doing so, we can obtain the estimate of the intercept and the penalised regression coefficients in one step.

```{r}
  # Create desing matrix with intercept
  X_scale_dm <- cbind(1, X_scale)

  # Compute cross-product matrix
  XtX <- crossprod(X_scale_dm)

  # Create penalty matrix
  pen <- lambda * diag(p + 1)
  pen[1] <- 0

  # Obtain standardized estimates
  bs_hat_r2 <- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)

  # Compare
  round(
          data.frame(
                  twostep = c(b0 = b0_hat_r, b = bs_hat_r),
                  onestep = drop(bs_hat_r2)
          ),
          3
  )

```
### Fit ridge regression with R packages
The most popular R package for regularised regression is `glmnet`.
Let's see how we can replicate the results we obtained with the manual approach with glmnet.
There are three important differences to consider:

- glmnet uses the biased version of variance estimation when scaling
- glmnet returns the unstandardized regression coefficients
- glmnet uses a different parametrization of lambda

To replicate the results we need to account for these.

#### Use the biased estimation of variance
First, let's obtain the results with the biased variance estimation:

```{r}
# Standardize X
X_scale <- sapply(1:p, function (j){
  muj <- mean(X[, j])
  sj <- sqrt( var(X[, j]) * (n-1) / n) # biased sd
  (X[, j] - muj) / sj                  # standardize
})

# Craete desing matrix with intercept
X_scale_dm <- cbind(1, X_scale)

# Compute cross-product matrix
XtX <- crossprod(X_scale_dm)

# Create penalty matrix
pen <- lambda * diag(p + 1)
pen[1] <- 0

# Obtain standardized estimates
bs_hat_r3 <- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)

# Print results
round(
      data.frame(
              manual = drop(bs_hat_r3)
      ),
      3
)

```

#### Return the unstandardized coefficients
Next, we need to revert these regression coefficients to their original scale.
```{r}
# Extract the original mean and standard deviations of all X variables
mean_x <- colMeans(X)
sd_x <- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version

# Revert to original scale
bs_hat_r4 <- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),
               bs_hat_r3[-1] / sd_x)

# Compare manual standardized and unstandardized results
round(
      data.frame(
              standardized = drop(bs_hat_r3),
              unstandardized = drop(bs_hat_r4)
      ),
      3
)

```
#### Adjust the parametrization of $\lambda$ for `glmnet`
Next, we can use `glmnet` to fit the ridge regression.
Pay attention to the fact that the value of lambda has been reparametrized for the `glmnet()` function.

```{r}
# Extract the original mean and standard deviations of y (for lambda parametrization)
mean_y <- mean(y)
sd_y <- sqrt(var(y) * (n - 1) / n)

# Compute the value glmnet wants for your target lambda
lambda_glmnet <- sd_y * lambda / n

```

#### Compare manual and `glmnet` ridge regression output

Finally, we can compare the results:
```{r}
# Fit glmnet
fit_glmnet_s <- glmnet(x = X,
                       y = y,
                       alpha = 0,
                       lambda = lambda_glmnet, # correction for how penalty is used
                       thresh = 1e-20)

bs_glmnet <- coef(fit_glmnet_s)

# Compare estimated coefficients
round(
      data.frame(
        manual = drop(bs_hat_r4),
        glmnet = drop(bs_glmnet)
      ),
      3
)

```
# TL;DR, just give me the code!
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```